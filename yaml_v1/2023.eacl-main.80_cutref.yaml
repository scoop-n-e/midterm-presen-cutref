# Narrative Understanding Survey - Paper Summary

metadata:
  title: "Event Temporal Relation Extraction with Bayesian Translational Model"
  authors:
    - "Xingwei Tan"
    - "Gabriele Pergola"
    - "Yulan He"
  year: 2023
  venue: "EACL 2023"
  paper_url: "https://aclanthology.org/2023.eacl-main.80/"

problem_statement:
  background: "Understanding events and how they evolve in time is beneficial for natural language understanding and numerous related tasks. Events often form complex structures through various temporal relations, which is challenging to track even for humans. The wide variety of linguistic expressions of temporal relations across different contexts adds to this complexity."
  
  gap_or_challenge: "Existing models for event temporal relation extraction lack a principled method to incorporate external knowledge. Most approaches are end-to-end neural architectures with limited use of commonsense knowledge. Although many temporal relations share linguistic similarities, they are characterized by unspoken knowledge that determines how temporal information is expressed, which varies greatly across domains. Current methods update event representations with knowledge features but lack a principled way of updating models' beliefs."
  
  research_objective: "This work introduces Bayesian-Trans, a Bayesian learning-based method that models temporal relation representations as latent variables and infers their values via Bayesian inference and translational functions. The approach aims to enhance the model's capability to encode and express uncertainty about predictions while incorporating prior temporal knowledge from external sources like ATOMIC."
  
  significance: "The proposed approach provides a principled methodology to incorporate commonsense knowledge and mitigate the lack of annotated data for event temporal relations. By combining Bayesian learning with translational models, the method enables better generalization and uncertainty quantification, avoiding overconfident predictions on out-of-distribution contexts."

tasks:
  - task_name: "Event Temporal Relation Extraction"
    
    task_overview: "This task evaluates a model's ability to predict the temporal relation type between two events given in text. The task requires determining whether one event happens before, after, or simultaneously with another event, or if their relation is vague. Unlike trigger-based extraction, events are represented as event pairs without specific trigger words. The task is challenging due to complex linguistic structures and implicit temporal information that requires commonsense knowledge to interpret correctly."
    
    input_description: "The model receives a context containing two events (head and tail events). Events are encoded from text sentences, and their contextual representations are extracted using pre-trained language models. The input may span multiple sentences with varying linguistic complexity."
    
    output_description: "The system outputs a temporal relation type from a predefined set (BEFORE, AFTER, EQUAL/SIMULTANEOUS, VAGUE). The prediction is made by computing scores for each relation type using a distance-based function on transformed event embeddings."

methodology:
  method_name: "Bayesian Translational Model (Bayesian-Trans)"
  
  approach_type: "neural"
  
  core_technique: "The proposed method combines Bayesian neural networks with translational models for temporal relation extraction. Event representations are encoded using COMET-BART, a BART model fine-tuned on ATOMIC knowledge graph. The translational parameters (relation matrices and translation vectors) are treated as latent variables following Gaussian distributions. These parameters are inferred through amortized variational inference, where the posterior distribution is conditioned on input events and the prior is derived from external knowledge graphs. The model uses MuRE (Multi-Relational Poincar√© Embeddings) as the translational model, applying relation-specific transformations to event embeddings. Maximum Mean Discrepancy (MMD) is used as regularization to ensure stable inference and prevent posterior collapse."
  
  base_models:
    - "COMET-BART"
    - "MuRE (Multi-Relational Embeddings)"
    - "RGCN (for prior learning)"
  
  key_innovations:
    - "Treating translational parameters as latent variables in a Bayesian framework for uncertainty quantification"
    - "Incorporating commonsense knowledge through learned priors from ATOMIC knowledge graph"
    - "Using variational inference with reparameterization trick for end-to-end differentiable learning"
    - "Employing Maximum Mean Discrepancy (MMD) as regularization for stable posterior inference"
    - "Combining knowledge-aware context encoding (COMET-BART) with Bayesian translational models"

datasets:
  - dataset_name: "MATRES"
    
    characteristics: "Multi-Axis Temporal RElations for Start-points dataset focusing on main time axes. Temporal relations between events are determined by their endpoints, resulting in consistent inter-annotator agreement. Contains densely annotated temporal relations."
    
    usage: "Primary dataset for training and evaluation of the temporal relation extraction model"
    
    size: "12,740 event relation pairs"
    
    domain: "news"
    
    is_new: false
    
    new_dataset_contribution: null
  
  - dataset_name: "TCR (Temporal and Causal Reasoning)"
    
    characteristics: "Follows the same annotation scheme as MATRES but with a much smaller number of event relation pairs. Used for cross-dataset evaluation to test generalization."
    
    usage: "Evaluation dataset to test model generalization"
    
    size: "2,646 event relation pairs"
    
    domain: "news"
    
    is_new: false
    
    new_dataset_contribution: null
  
  - dataset_name: "TimeBank-Dense (TBD)"
    
    characteristics: "Densely annotated dataset focusing on the most salient events. Provides 6 event temporal relations including INCLUDE and ISINCLUDED relations not present in other datasets."
    
    usage: "Additional evaluation dataset with different annotation scheme"
    
    size: "12,715 event relation pairs"
    
    domain: "news"
    
    is_new: false
    
    new_dataset_contribution: null

evaluation:
  evaluation_strategy: "Models are trained on MATRES training set and evaluated on MATRES test set, TCR, and TimeBank-Dense. Evaluation uses F1 score for MATRES and TCR following specific definitions, and micro-F1 for TimeBank-Dense. The model is compared against baselines with and without commonsense knowledge incorporation. Additional analyses include ablation studies, prior comparison, and uncertainty quantification using entropy and mutual information metrics."
  
  main_results: "Bayesian-Trans achieves 82.7% F1 on MATRES, outperforming the previous best model (HGRU + knowledge) by 2.2%. On TCR, it obtains 86.1% F1, surpassing the best baseline by 2.4%. On TimeBank-Dense, it achieves 65.0% micro-F1, improving over UAST by 0.7%. The model shows significant gains especially when incorporating RGCN-learned priors from ATOMIC (82.7% vs 81.2% with standard Gaussian prior on MATRES). Ablation studies confirm that Bayesian learning provides 0.9-2.5% improvement over non-Bayesian variants."
  
  metrics_used:
    - "F1-score"
    - "Micro-F1"
    - "Precision"
    - "Recall"
    - "Entropy (uncertainty)"
    - "Mutual Information (model uncertainty)"
  
  human_evaluation_summary: null

results_analysis:
  key_findings:
    - "COMET-BART encoding provides better contextual representations than RoBERTa for temporal relations"
    - "Bayesian learning naturally incorporates prior knowledge and improves performance by 0.9-2.5% over non-Bayesian variants"
    - "RGCN-learned priors from ATOMIC show the highest activation and best performance compared to standard Gaussian or MuRE priors"
    - "The model exhibits reasonable uncertainty quantification, showing higher confidence on simpler linguistic structures"
    - "Error rate increases by 19% when events reside in different sentences"
  
  ablation_summary: "Removing Bayesian learning reduces F1 by 0.9-1.8% on MATRES and 0.2-2.5% on TBD. COMET-BART outperforms RoBERTa as context encoder. MuRE alone without Bayesian framework shows no improvement over simple MLP. RGCN-learned priors significantly outperform standard Gaussian priors."

contributions:
  main_contributions:
    - "We formulate a novel Bayesian translational model for event temporal relation extraction, treating translational parameters as latent variables inferred through variational inference. This enables principled uncertainty quantification and knowledge incorporation."
    - "We devise and explore three different priors (standard Gaussian, MuRE-based, RGCN-based) under the Bayesian framework to study effective knowledge incorporation. RGCN-learned priors from ATOMIC show superior performance."
    - "Experimental results on three benchmarks demonstrate state-of-the-art performance. The model achieves 82.7% F1 on MATRES, 86.1% on TCR, and 65.0% micro-F1 on TimeBank-Dense, with comprehensive analyses of uncertainty quantification and prior effectiveness."
  
  limitations:
    - "The approach takes event pairs as input; error rate increases by 19% for events in different sentences"
    - "Current work only handles temporal relations; could be extended to causal or hierarchical relations"
    - "Limited to ATOMIC as knowledge source; could be extended to multiple sources"
    - "Not all translational models were exhaustively investigated"

narrative_understanding_aspects:
  - "Reading Comprehension"  # Understanding temporal relations between events in text
  - "Narrative Consistency Check"  # Ensuring temporal consistency in event sequences
  - "Plot / Storyline Extraction"  # Extracting temporal structure of events

keywords:
  - "event temporal relation"
  - "Bayesian learning"
  - "translational models"
  - "variational inference"
  - "commonsense knowledge"
  - "ATOMIC"
  - "uncertainty quantification"
  - "MuRE"
  - "COMET-BART"

notes: "This work uniquely combines Bayesian learning with translational models for temporal relation extraction. The approach is particularly notable for its principled incorporation of commonsense knowledge through learned priors and its ability to quantify prediction uncertainty."