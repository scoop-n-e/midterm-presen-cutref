# Narrative Understanding Survey - Paper Summary

metadata:
  title: "Reading Subtext: Evaluating Large Language Models on Short Story Summarization with Writers"
  authors:
    - "Melanie Subbiah"
    - "Sean Zhang"
    - "Lydia B. Chilton"
    - "Kathleen McKeown"
  year: 2024
  venue: "TACL"
  paper_url: "https://doi.org/10.1162/tacl_a_00702"

problem_statement:
  background: "Narrative is a fundamental part of how we communicate and make sense of our experiences. Understanding narrative is a necessary skill for Large Language Models to engage with the subtleties of human experience and communication. Fiction short stories present challenges as they don't follow clear formats, may use non-linear timelines, hint at things abstractly, deliberately mislead readers, and use creative language varieties."
  
  gap_or_challenge: "Current evaluations of narrative summarization on long documents are scarce due to: 1) Narrative text being either in public domain (likely in LLM training data) or under copyright, 2) Lack of reliable automatic metrics and complications with human evaluation (expensive and time-consuming). Most LLM evaluations focus on literal comprehension rather than interpretive understanding of themes and subtext."
  
  research_objective: "This work evaluates recent LLMs (GPT-4, Claude-2.1, LLama-2-70B) on summarizing unpublished short stories provided by experienced creative writers. The study focuses on interpreting themes and meaning in addition to capturing plot, examining narrative understanding through quantitative and qualitative analysis grounded in narrative theory."
  
  significance: "Working directly with writers ensures stories are unseen by models and enables informed evaluations from authors themselves. This approach demonstrates the mutual benefit of collaborating with communities who have valuable data, as increasing amounts of online content are consumed or generated by models."

tasks:
  - task_name: "Short Story Summarization"
    
    task_overview: "This task evaluates LLMs on summarizing fiction short stories that can be lengthy (up to 10,000 tokens), include nuanced subtext, scrambled timelines, and creative language. Models must generate summaries that capture plot points, maintain faithfulness to the story, provide coherent narrative flow, and potentially analyze themes and meaning. The task requires understanding both explicit content and implicit subtext."
    
    input_description: "Unpublished short stories from experienced creative writers. Stories range from 1,854 to 8,126 tokens (average 4,327), categorized as short (<3,300 tokens), medium (3,300-6,600 tokens), or long (6,600-9,900 tokens). Stories may include unreliable narrators, non-linear timelines, detailed subplots, and varied reading levels."
    
    output_description: "Summaries approximately 400 words in length that should demonstrate coverage of important plot points, faithfulness to story details, coherent narrative flow, and analysis of themes/takeaways. Models generate summaries using different approaches: direct summarization for long-context models or chunk-then-summarize for shorter context windows."

methodology:
  method_name: "Writer-Based Evaluation Framework"
  
  approach_type: "hybrid"
  
  core_technique: "The framework combines multiple evaluation approaches. For summary generation: GPT-4 and Claude-2.1 use single-prompt summarization leveraging long context windows. Llama-2-70B uses hierarchical chunk-then-summarize approach for longer stories (1-4 chunks based on length). For evaluation: Writers provide span-level error categorization (7 participated, 69/75 summaries annotated), summary-level Likert ratings (1-4 scale) across four attributes (coverage, faithfulness, coherence, analysis), and ranking of three model summaries. Error categories based on narrative theory include: Coverage (insignificant, vague), Faithfulness (feeling, character, causation, action, setting), Coherence (inconsistent, abrupt transition, missing context, repetition), Analysis (unsupported). Story-level style effects analyzed using Genette's narrative model (narrating, story, discourse) and Flesch-Kincaid readability scores."
  
  base_models:
    - "GPT-4 (November 2023 update)"
    - "Claude-2.1"
    - "Llama-2-70B-chat"
  
  key_innovations:
    - "First evaluation of LLM narrative summarization on unpublished stories unseen during training"
    - "Span-level, summary-level, and story-level evaluation grounded in narrative theory"
    - "Expert writer evaluation enabling rapid, informed assessment of faithfulness and thematic analysis"
    - "Exploration of narrative interpretation beyond literal comprehension"
    - "Analysis of how narrative elements (unreliable narrators, non-linear plots) affect summary quality"

datasets:
  - dataset_name: "Unpublished Short Stories Collection"
    
    characteristics: "25 original short stories from 9 experienced creative writers (8/9 have writing degrees, 7/9 previously published). Stories average 4,327 tokens with perplexity scores 9.2-21.6 (indicating unfamiliarity to models). Diverse narrative styles including unreliable narrators, flashbacks, detailed subplots. Writers submitted 1-5 stories each, evaluated summaries of their own work."
    
    usage: "Primary dataset for evaluating narrative summarization capabilities"
    
    size: "25 stories"
    
    domain: "fiction"
    
    is_new: true
    
    new_dataset_contribution: "First dataset of unpublished fiction stories with expert author evaluations. Unlike existing datasets using public domain texts (likely memorized by LLMs) or recent books (may be discussed in training data), this ensures true out-of-distribution evaluation. Includes author-verified labels for narrative elements and expert evaluation of summary quality across multiple dimensions."

evaluation:
  evaluation_strategy: "Multi-faceted evaluation combining writer judgments with automatic metrics. Writers rate summaries on 4-point Likert scale for coverage, faithfulness, coherence, and analysis. Span-level error annotation uses narrative theory-based categories. Pairwise ranking of three model summaries. Automatic metrics tested include ROUGE, BERTScore, AlignScore, UniEval, MiniCheck, BooookScore, FABLES. LLM-based evaluation using GPT-4 and Claude as judges. Statistical analysis using Wilcoxon signed-rank tests and Pearson correlation."
  
  main_results: "GPT-4 achieves highest average scores (3.38/4) but perfect scores only 54% of time. Faithfulness most challenging - only 44% perfect for GPT-4, 30% Claude, 8% Llama. All models make faithfulness errors in >50% of summaries. Llama significantly worse (p<.05) with average 2.54/4. GPT-4 and Claude ranked equally by writers despite score differences. Models struggle with specificity and subtext interpretation. 56% of GPT-4 summaries show some thematic analysis capability. Automatic metrics poorly correlate with writer scores (GPT-4 judge r=.46 coverage, r=.37 faithfulness; Claude judge shows no correlation)."
  
  metrics_used:
    - "Likert scale ratings (1-4) for four attributes"
    - "Span-level error counts by category"
    - "Pairwise preference rankings"
    - "ROUGE, BERTScore for reference-based evaluation"
    - "AlignScore, UniEval, MiniCheck for faithfulness"
    - "BooookScore, FABLES for LLM-based evaluation"
    - "Perplexity scores for training data similarity"
  
  human_evaluation_summary: "9 experienced writers (8 with writing degrees, 7 published) evaluated 75 summaries. Writers completed ratings in 5-10 minutes per story (vs 60-90 minutes for non-authors). Moderate inter-annotator agreement on error categories (Fleiss-Kappa 0.51 for faithfulness categories). Writers identified 352 total errors across models. Qualitative feedback highlighted issues with subtext, normative assumptions, and unique phrase copying."

results_analysis:
  key_findings:
    - "GPT-4 and Claude produce excellent summaries only ~50% of time"
    - "Vagueness (64 errors) and unsupported analysis (104 errors) are major issues across models"
    - "Most faithfulness errors involve character feelings (23) and actions (39)"
    - "Unreliable narrators significantly harder for all models (average scores drop)"
    - "No correlation between story length and summary quality for long-context models"
    - "Reading level (Flesch-Kincaid) doesn't affect summary quality"
    - "LLM judges cannot replace skilled human evaluators - poor correlation with writer scores"
  
  ablation_summary: "Style analysis shows unreliable narrators reduce scores for all models (GPT-4: 3.45→3.28, Claude: 3.29→2.89, Llama: 2.60→2.45). Chunk-then-summarize approach causes character conflation errors in Llama. Coverage errors more common in Claude (missing subplots). GPT-4 copies more unique phrasing (5.72 avg longest common substring vs 4.16 Claude)."

contributions:
  main_contributions:
    - "First evaluation of LLM narrative summarization on truly unseen fiction stories through collaboration with creative writers, avoiding training data contamination."
    - "Comprehensive evaluation framework combining span-level error categorization based on narrative theory, summary-level ratings, and story-level style analysis."
    - "Evidence that even best LLMs make significant errors in >50% of summaries, struggling particularly with subtext interpretation, unreliable narrators, and character feelings/actions."
    - "Demonstration that LLM-based evaluation cannot replace expert human judgment for narrative tasks, with poor correlation between model and writer scores."
  
  limitations:
    - "Limited to 25 stories from 9 writers due to cost and availability constraints"
    - "Stories limited to 10,000 tokens to fit model context windows"
    - "Evaluation focuses on English language fiction only"
    - "Simple prompting strategies without extensive prompt engineering"
    - "Potential for individual writer bias in evaluations"

narrative_understanding_aspects:
  - "other"  # Narrative summarization with theme/subtext interpretation

keywords:
  - "narrative summarization"
  - "fiction understanding"
  - "subtext interpretation"
  - "unreliable narrators"
  - "writer evaluation"
  - "faithfulness errors"
  - "theme analysis"
  - "narrative theory"
  - "out-of-distribution evaluation"

notes: "Code and evaluation data available at https://github.com/melaniesubbiah/reading-subtext. Study demonstrates importance of collaborating with domain experts for authentic evaluation beyond training data."